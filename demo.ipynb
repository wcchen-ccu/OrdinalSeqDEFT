{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a568358",
   "metadata": {},
   "source": [
    "# Ordinal SeqDEFT\n",
    "\n",
    "This is a demo of the density estimation method, **Ordinal SeqDEFT** (standing for Ordinal Sequence Density Estimation using Field Theory). It includes the following subjects:\n",
    "\n",
    "1. Preliminary preparation\n",
    "2. Data importation\n",
    "3. MAP estimation\n",
    "4. Cross validation\n",
    "5. Getting the result\n",
    "6. Computing associations\n",
    "7. Making visualization\n",
    "\n",
    "As an example, we use it to compute a small problem that involves only 27 sequences. For problems with tens of thousands of sequences, the calculation can be readily done on a typical laptop computer. For problems larger than that, deploying the computations on a workstation or a cluster can greatly facilitate the calculation. Even with parallel computation, however, the maximum number of sequences that the algorithm can run on is about a few millions.\n",
    "\n",
    "**Reference**\n",
    "- Wei-Chia Chen, Juannan Zhou, and David M. McCandlish, [Density estimation for ordinal biological sequences and its applications](https://arxiv.org/abs/2404.11228), arXiv:2404.11228 (2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bca1b5",
   "metadata": {},
   "source": [
    "Import Python packages that will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c450b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55fafae",
   "metadata": {},
   "source": [
    "Import Ordinal SeqDEFT functions from the file `functions.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317e0967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import preliminary_preparation, import_data, trace_MAP_curve, \\\n",
    "                      compute_log_likelihoods, make_visualization, get_nodes, get_edges, \\\n",
    "                      compute_rms_log_p_association, compute_log_p_associations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dab9a01",
   "metadata": {},
   "source": [
    "Suppose we have a set of ordinal sequence data. Each sequence has 3 sites, and each site can take on 3 possible elements, say, `1`, `2`, `3`. In ordinal SeqDEFT, the sequence space is represented by a 3 x 3 x 3 grid graph, as shown below:\n",
    "\n",
    "<img src='grid.png' height='350' width='350'>\n",
    "\n",
    "In the figure each node corresponds to a sequence, and each edge corresponds to a single point mutation. The number of times that a sequence was observed is represented by the size of the sphere. The data is stored in the file `data.txt`. Our goal is to estimate the probability distribution from which the sequences were drawn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f00d1d",
   "metadata": {},
   "source": [
    "### 1. Preliminary preparation\n",
    "Specify number of elements `alpha`, number of sites `l`, and order of the smoothness operator `P` with the function \n",
    "``` python\n",
    "preliminary_preparation(alpha, l, P, parameters_only=False, time_it=False)\n",
    "```\n",
    "This will set up the stuff that will be needed throughout the calculation, including kernel basis of the smoothness operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c7e367",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, l, P = 3, 3, 2\n",
    "\n",
    "# ---\n",
    "\n",
    "preliminary_preparation(alpha, l, P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414c0a25",
   "metadata": {},
   "source": [
    "### 2. Data importation\n",
    "Import data with the function\n",
    "```python\n",
    "import_data(path, coding_dict, ignore_sites=None)\n",
    "```\n",
    "Data should be prepared in a count-data format as the example file `data.txt`, in which sequences are stored in the first column and counts are stored in the second column, separated by one or more whitespace characters. Unobserved sequences need not be listed in the file. Also, we should specify how to encode elements. In Ordinal SeqDEFT, elements from small to large are encoded as `0`, `1`, `2`, ... and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e077a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data.txt'\n",
    "coding_dict = {'1':0, '2':1, '3':2}\n",
    "\n",
    "# ---\n",
    "\n",
    "data_dict = import_data(path, coding_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34e1290",
   "metadata": {},
   "source": [
    "The function will return a dictionary `data_dict` that contains total number of sequences `N` in the data and frequencies of each possible sequence `R`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aab591",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b906237",
   "metadata": {},
   "source": [
    "### 3. MAP estimation\n",
    "For each value of the hyperparameter $a$ between zero and infinity, find the corresponding MAP solution of $\\phi$. This is done with the function\n",
    "```python\n",
    "trace_MAP_curve(data_dict, resolution=0.1, num_a=20, fac_max=1, fac_min=1e-3, options=None, scale_by=1)\n",
    "```\n",
    "The function will first find a large value of $a$ to substitute infinity and a small value of $a$ to substitute zero. This is achieved by increasing (decreasing) the value of $a$ from `s * fac_max` (`s * fac_min`), where `s` is the number of $P$-dimensional hypercubes, until the geodesic distance between the corresponding distribution and $Q_\\infty$ ($Q_0$) is less than `resolution`. Then `num_a` values of $a$, including the two extremes found above, are selected by a geometric partition. These values, together with zero and infinity, represent the entire MAP curve.\n",
    "\n",
    "Also, we can control the minimization procedure by passing parameters through the argument `options`. From our experience, using gradient (controlled by `gtol`) rather than function value (controlled by `ftol`) as the stopping criterion is safer as the latter may result in a premature termination. The stopping criterion based on function value can be \"turned off\" by setting the parameter `ftol` to an extremely small value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a987dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_a = 20\n",
    "options = {'ftol':1e-100, 'gtol':1e-5, 'maxiter':50000, 'maxfun':100000}\n",
    "\n",
    "# ---\n",
    "\n",
    "df_map = trace_MAP_curve(data_dict, num_a=num_a, options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bd3459",
   "metadata": {},
   "source": [
    "The function will return a dataframe `df_map` that contains the values of $a$ and the corresponding field $\\phi_a$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1a2783",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4244fbf8",
   "metadata": {},
   "source": [
    "One way to check if the MAP solutions $\\phi_a$'s were solved accurately is to see if they satisfy the condition: $\\sum e^{-\\phi_a} = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d83420",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_map)):\n",
    "    a, phi = df_map.loc[i, ['a','phi']]\n",
    "    print('sum = %f at a = %.2f' % (np.sum(np.exp(-phi)), a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6555c2",
   "metadata": {},
   "source": [
    "### 4. Cross validation\n",
    "To find the optimal value of $a$, we employ $k$-fold cross validation. This is done by the function\n",
    "```python\n",
    "compute_log_likelihoods(data_dict, df_map, cv_fold=5, random_seed=None, options=None, scale_by=1)\n",
    "```\n",
    "As in the part of MAP estimation, the argument `options` is used to pass parameters controlling the minimization procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62485182",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cv_fold = 5\n",
    "random_seed = 0\n",
    "options = {'ftol':1e-100, 'gtol':1e-5, 'maxiter':50000, 'maxfun':100000}\n",
    "\n",
    "# ---\n",
    "\n",
    "df_map = compute_log_likelihoods(data_dict, df_map, cv_fold, random_seed, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34cccc0",
   "metadata": {},
   "source": [
    "The function will return the same dataframe `df_map` with an additional column recording the cross-validated log likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63848be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c4adec",
   "metadata": {},
   "source": [
    "### 5. Getting the result\n",
    "Plot cross-validated log likelihood $\\log L$ versus the hyperparameter $a$. Note that $\\log L = -\\infty$ at $a = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941042ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa, log_Ls = df_map['a'].values, df_map['log_L'].values\n",
    "i_star = log_Ls.argmax()\n",
    "a_star, max_log_L = aa[i_star], log_Ls[i_star]\n",
    "\n",
    "# ---\n",
    "\n",
    "plt.figure(figsize=(6,4.5))\n",
    "\n",
    "with np.errstate(divide='ignore'):\n",
    "    plt.scatter(np.log10(aa), log_Ls, color='blue', zorder=1)\n",
    "    plt.scatter(4, log_Ls[-1], color='blue', zorder=1)\n",
    "    plt.scatter(np.log10(a_star), max_log_L, color='red', zorder=2, label=r'$a^*$ = %.1f'%a_star)\n",
    "\n",
    "plt.xlim(-2, 4)\n",
    "plt.ylim(-71, -66)\n",
    "plt.xticks([-2, -1, 0, 1, 2, 3, 4], ['-2', '-1', '0', '1', '2', '3', r'$\\infty$'], fontsize=16)\n",
    "plt.yticks([-71, -70, -69, -68, -67, -66], ['-71', '-70', '-69', '-68', '-67', '-66'], fontsize=16)\n",
    "plt.xlabel(r'$\\log_{10} (a)$', fontsize=20)\n",
    "plt.ylabel(r'$\\log \\ \\! (L)$', fontsize=20)\n",
    "plt.legend(fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3943bb",
   "metadata": {},
   "source": [
    "From the plot we can see that the optimal value of $a$ is about $1.1$, and the corresponding distribution, denoted $Q^*$, is what we want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59b910c",
   "metadata": {},
   "source": [
    "We can compare $Q^*$ with the observed frequency $Q_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f6fddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "phis = df_map['phi'].values\n",
    "phi_star = phis[i_star]\n",
    "Q_star = np.exp(-phi_star) / np.sum(np.exp(-phi_star))\n",
    "phi_0 = phis[0]\n",
    "Q_0 = np.exp(-phi_0) / np.sum(np.exp(-phi_0))\n",
    "\n",
    "# ---\n",
    "\n",
    "plt.figure(figsize=(5.5,5.5))\n",
    "plt.plot([0, 0.105], [0, 0.105], color='grey', linewidth=1, linestyle='--', zorder=1)\n",
    "plt.scatter(Q_0, Q_star, color='blue', s=50, alpha=0.3, zorder=2)\n",
    "plt.xlim(0, 0.105)\n",
    "plt.ylim(0, 0.105)\n",
    "plt.xticks([0, 0.02, 0.04, 0.06, 0.08, 0.10], ['0', '0.02', '0.04', '0.06', '0.08', '0.10'], fontsize=16)\n",
    "plt.yticks([0, 0.02, 0.04, 0.06, 0.08, 0.10], ['0', '0.02', '0.04', '0.06', '0.08', '0.10'], fontsize=16)\n",
    "plt.xlabel(r'$Q_0$', fontsize=20)\n",
    "plt.ylabel(r'$Q$*', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8389848",
   "metadata": {},
   "source": [
    "Looks great! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90514d98",
   "metadata": {},
   "source": [
    "On the other hand, the maximum entropy estimate $Q_\\infty$ (which is an additive model in this case) gives bad results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b44172",
   "metadata": {},
   "outputs": [],
   "source": [
    "phis = df_map['phi'].values\n",
    "phi_inf = phis[-1]\n",
    "Q_inf = np.exp(-phi_inf) / np.sum(np.exp(-phi_inf))\n",
    "phi_0 = phis[0]\n",
    "Q_0 = np.exp(-phi_0) / np.sum(np.exp(-phi_0))\n",
    "\n",
    "# ---\n",
    "\n",
    "plt.figure(figsize=(5.5,5.5))\n",
    "plt.plot([0, 0.105], [0, 0.105], color='grey', linewidth=1, linestyle='--', zorder=1)\n",
    "plt.scatter(Q_0, Q_inf, color='blue', s=50, alpha=0.3, zorder=2)\n",
    "plt.xlim(0, 0.105)\n",
    "plt.ylim(0, 0.105)\n",
    "plt.xticks([0, 0.02, 0.04, 0.06, 0.08, 0.10], ['0', '0.02', '0.04', '0.06', '0.08', '0.10'], fontsize=16)\n",
    "plt.yticks([0, 0.02, 0.04, 0.06, 0.08, 0.10], ['0', '0.02', '0.04', '0.06', '0.08', '0.10'], fontsize=16)\n",
    "plt.xlabel(r'$Q_0$', fontsize=20)\n",
    "plt.ylabel(r'$Q_\\infty$', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903e38ab",
   "metadata": {},
   "source": [
    "### 6. Computing associations\n",
    "We can study associations among the sequence sites with the inferred probability distribution $Q^*$. \n",
    "\n",
    "First, we can compute the \"association specturm\" which tells us the association strength of each order. This is done with the function\n",
    "```python\n",
    "compute_rms_log_p_association(phi, p)\n",
    "```\n",
    "with $p = 1, 2, \\dots, \\ell$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fe94d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rms_log_Aps = np.zeros(l)\n",
    "for p in range(1, l+1):\n",
    "    rms_log_Ap = compute_rms_log_p_association(phi_star, p)\n",
    "    rms_log_Aps[p-1] = rms_log_Ap\n",
    "    \n",
    "# ---\n",
    "\n",
    "plt.figure(figsize=(6,4.5))\n",
    "plt.plot(range(1,l+1), np.exp(rms_log_Aps), color='blue')\n",
    "plt.scatter(range(1,l+1), np.exp(rms_log_Aps), color='blue')\n",
    "plt.xlim(0.75, 3.25)\n",
    "plt.ylim(0, 30)\n",
    "plt.xticks([1, 2, 3], ['1', '2', '3'], fontsize=16)\n",
    "plt.yticks([0, 5, 10, 15, 20, 25, 30], ['0', '5', '10', '15', '20', '25', '30'], fontsize=16)\n",
    "plt.xlabel(r'Order, $p$', fontsize=20)\n",
    "plt.ylabel(r'Effective $A^{(p)}$', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a834d2ee",
   "metadata": {},
   "source": [
    "Each value of effective $A^{(p)}$ in the plot is equal to the exponential of the root-mean-square value of the $\\log A^{(p)}$'s conditional on all possible backgrounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f505c6",
   "metadata": {},
   "source": [
    "On the other hand, the function\n",
    "```python\n",
    "compute_log_p_associations(phi, sites_dict, condition_dict={}, coding_dict=None)\n",
    "```\n",
    "allows us to look into the associations. \n",
    "\n",
    "For example, to compute the association between the 1st site and the 2nd site (without specifying \"mutations\" at them), we do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d0d437",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_dict = {0: None, # 1st site, with all possible mutations\n",
    "              1: None} # 2nd site, with all possible mutations\n",
    "\n",
    "# ---\n",
    "\n",
    "df_log_Aps = compute_log_p_associations(phi_star, sites_dict, coding_dict={'1':0, '2':1, '3':2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48812f5e",
   "metadata": {},
   "source": [
    "The function will return a dataframe `df_log_Aps`, recording the values of $\\log A^{(p)}$ along with the associated sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc47a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log_Aps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0dfd35",
   "metadata": {},
   "source": [
    "To compute the association between the 1st site and the 2nd site with the former \"mutating\" from `1` to `2` and the latter \"mutating\" from `2` to `3`, we do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08b8784",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_dict = {0: ['1', '2'], # 1st site, mutating from '1' to '2'\n",
    "              1: ['2', '3']} # 2nd site, mutating from '2' to '3'\n",
    "\n",
    "# ---\n",
    "\n",
    "df_log_Aps = compute_log_p_associations(phi_star, sites_dict, coding_dict={'1':0, '2':1, '3':2})\n",
    "df_log_Aps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adeb1f8",
   "metadata": {},
   "source": [
    "We can even specify the backgrounds in which the \"mutations\" take place. Let us use the same example and fix the 3rd site at `1` and `3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7027875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_dict = {0: ['1', '2'], # 1st site, mutating from '1' to '2'\n",
    "              1: ['2', '3']} # 2nd site, mutating from '2' to '3'\n",
    "condition_dict = {2: ['1', '3']} # 3rd site, fixed at '1' and '3'\n",
    "\n",
    "# ---\n",
    "\n",
    "df_log_Aps = compute_log_p_associations(phi_star, sites_dict, condition_dict, coding_dict={'1':0, '2':1, '3':2})\n",
    "df_log_Aps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3757a4",
   "metadata": {},
   "source": [
    "In a similar manner, the function can be used to compute associations of any order, with or without specifying \"mutations\", conditional on certain backgrounds or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28419378",
   "metadata": {},
   "source": [
    "### 7. Making visualization\n",
    "There are various dimensionality reduction techniques that can be used to visualize high-dimensional objects like $Q^*$. We employ the one based on an evolutionary model from [D. M. McCandlish (2011)](https://academic.oup.com/evolut/article/65/6/1544/6854390). This is done with the function\n",
    "```python\n",
    "make_visualization(Q, markov_chain, K=20, tol=1e-9, reuse_Ac=False, path='sparse_matrix/Ac/')\n",
    "```\n",
    "The input distribution `Q` will be used to construct the rate matrix of an imaginary Markov chain, whose first `K` eigenvectors will be solved and used as visualization coordinates. The accuracy of this computation is controlled by the parameter `tol`. The results are stored in two dataframes `df_visual` and `df_check`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6758b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = Q_star\n",
    "markov_chain = 'evolutionary'\n",
    "K = 10\n",
    "\n",
    "# ---\n",
    "\n",
    "df_visual, df_check = make_visualization(Q, markov_chain, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30ec768",
   "metadata": {},
   "source": [
    "The dataframe `df_visual` stores the visualization coordinates. Note that the zeroth eigenvalue is supposed to be zero and the corresponding coordinates are supposed to be NAN's; both will not be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faafde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visual.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7afd6c",
   "metadata": {},
   "source": [
    "The dataframe `df_check` contains information about the computation. In general, `colinearity` should be $1.0$ and `max_difference` should be as small as possible, say, around $10^{-16}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f324b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386a5dc9",
   "metadata": {},
   "source": [
    "The reciprocal of the absolute value of each eigenvalue in `df_visual` is proportional to the variance explained in that direction. So let us first take a look at the spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822d22e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = range(1, K), 1/abs(df_visual['eigenvalue'].values[1:])\n",
    "\n",
    "# ---\n",
    "\n",
    "plt.figure(figsize=(6,4.5))\n",
    "plt.scatter(x, y, color='blue')\n",
    "plt.xlim(0, K)\n",
    "plt.ylim(0, )\n",
    "plt.xticks([0, 2, 4, 6, 8, 10], ['0', '2', '4', '6', '8', '10'], fontsize=16)\n",
    "plt.yticks([0, 0.5, 1, 1.5, 2, 2.5, 3], ['0', '0.5', '1.0', '1.5', '2.0', '2.5', '3.0'], fontsize=16)\n",
    "plt.xlabel(r'$k$', fontsize=20)\n",
    "plt.ylabel('1 / |Eigenvalue|', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a7ed42",
   "metadata": {},
   "source": [
    "It seems reasonable to visualize $Q^*$ with the first two or three directions. To do that, we need to get the nodes and edges with the functions\n",
    "```python\n",
    "get_nodes(df_visual, kx, ky, xflip=1, yflip=1)\n",
    "get_edges(df_visual, kx, ky, xflip=1, yflip=1)\n",
    "```\n",
    "The parameters `xflip` and `yflip` can be used to deform the graph via transformations `x = x * xflip` and `y = y * yflip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bb4dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "kx, ky = 1, 2\n",
    "\n",
    "# ---\n",
    "\n",
    "df_nodes = get_nodes(df_visual, kx, ky)\n",
    "df_edges = get_edges(df_visual, kx, ky)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ddf3b6",
   "metadata": {},
   "source": [
    "Now we can make the plot. We color-code each node with the inferred probability of the corresponding sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a26792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "color_by = np.log10(Q_star) \n",
    "vmin, vmax = np.floor(np.log10(Q_star).min()), np.ceil(np.log10(Q_star).max())\n",
    "\n",
    "# ---\n",
    "\n",
    "plt.figure(figsize=(8,6.5))\n",
    "\n",
    "# Plot edges\n",
    "edges = df_edges['edge'].values\n",
    "ln_coll = LineCollection(edges, color='grey', linewidths=1, alpha=0.8, zorder=1)\n",
    "ax = plt.gca()\n",
    "ax.add_collection(ln_coll)\n",
    "plt.draw()\n",
    "\n",
    "# Plot nodes\n",
    "nodes_x, nodes_y, nodes_c = df_nodes['x'].values, df_nodes['y'].values, color_by\n",
    "plt.scatter(nodes_x, nodes_y, c=nodes_c, vmin=vmin, vmax=vmax, cmap='jet', s=30, zorder=2)\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label(r'$\\log_{10} \\ \\! (Q^*)$', size=20)\n",
    "\n",
    "plt.xlabel('Diffusion Axis %d'%kx, fontsize=20)\n",
    "plt.ylabel('Diffusion Axis %d'%ky, fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ba1bac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
